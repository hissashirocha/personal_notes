**Important notes about airflow

RUN tasks individually
- airflow tasks test dag_name task_name 2019-01-01 (date in the past)

RUN Backfill
- airflow backfill -s 2019-01-01 -e 2019-01-05 --rerun_failed_tasks -B (process dates backwards) dag_name

DEFINING Number of Workers
* ideally: number of cores * 2 + 1

DEBUGGING
- change logging_level parameter in the airflow.cfg and check the webserver logs: sudo docker logs -f XXXXXXXXX

** Dag level: dagrun_timeout, sla_miss, on_failure and on_sucess callback functions
** Task level: email, retry params, callbacks

- it's possible to check logs generated by each dag inside the container /logs/scheduler/ ....

PARALLELISM
- parallelism=3, means it will run at most 3 tasks in parallel (not necessarily for the same DAG run). If set to 0, the parallelism is only limited by the cores/processes available in the machine
- max_active_runs_per_dag=1, mean it will only have one dag run at a time (if more, several task from different runs may run at the same time)
- dag_concurrency=2, means only 2 tasks can run simultaneously per dag run (if combinaded with max=1, even with parallelism set to 3, only two tasks will run in parallel, because even though it may have 3 tasks running in total, we set one 1 dag run per time, and only 2 tasks per dag run)
** If tasks are not too I/O bound, you may set parallelism to 'number of cores - 1'
- to add new workers, just use command 'sudo docker-compose -f docker-compose.yml scale workers=x' where X goes the number of total workers wanted

